# Step 1.2 — Ingest Service (Chunking + Indexing)

## Objective

Build a service that reads the raw documentation files downloaded in Step 1.1, splits them into meaningful chunks, generates embeddings, attaches rich metadata, and indexes everything into ChromaDB.

## Input

Raw documentation files stored in `raw/docs/{library}/` (Markdown, MDX, RST).

## Output

ChromaDB collection `technical_docs` populated with chunks, each containing:
- **document**: The chunk text
- **embedding**: Generated by `all-MiniLM-L6-v2`
- **metadata**: Structured info about the chunk
- **id**: Deterministic, unique identifier

---

## Implementation: `src/services/ingest.py`

### Class: `IngestService`

```python
class IngestService:
    """Processes raw docs into chunked, embedded documents in ChromaDB."""

    def __init__(self, db_handler: ChromaDBHandler, raw_docs_path: str = "raw/docs"):
        self.db = db_handler
        self.raw_docs_path = raw_docs_path

    def read_file(self, file_path: str) -> str:
        """Read a file and return its content as string.
        
        Handle UTF-8 encoding. Skip binary files gracefully.
        For .mdx files: strip JSX components (anything between < /> that isn't standard markdown).
        For .rst files: basic RST-to-text cleanup (remove directives like `.. code-block::`, 
        convert RST headers to markdown-style, remove role markup like `:ref:`).
        """
        ...

    def chunk_markdown(self, content: str, source_file: str, library: str) -> list[dict]:
        """Split markdown content into chunks based on headers.
        
        Strategy:
        1. Split by headers (##, ###). Keep # (h1) as the document title.
        2. Each chunk = one section (from one header to the next).
        3. If a section is too long (>1000 tokens ~= >4000 chars), split it further 
           by paragraphs (double newline) with overlap of 200 chars.
        4. If a section is too short (<100 chars), merge it with the next section.
        5. Preserve code blocks intact — never split in the middle of a ``` block.
        
        Each chunk dict:
        {
            "text": "The chunk content",
            "metadata": {
                "library": "langchain",        # Which library
                "source_file": "docs/concepts/architecture.md",  # Relative path
                "section": "Architecture Overview",  # H1 or filename
                "subsection": "Components",    # H2/H3 header of this chunk
                "chunk_index": 0,              # Order within the file
                "total_chunks": 5,             # Total chunks from this file
                "char_count": 1234,            # Length of chunk text
                "has_code": true,              # Whether chunk contains code blocks
            }
        }
        
        Returns list of chunk dicts.
        """
        ...

    def generate_chunk_id(self, library: str, source_file: str, chunk_index: int) -> str:
        """Generate a deterministic ID for a chunk.
        
        Format: "{library}::{source_file}::chunk_{chunk_index}"
        Example: "langchain::concepts/architecture.md::chunk_0"
        
        This ensures idempotency — running ingest twice won't duplicate data.
        """
        ...

    async def ingest_library(self, library: str) -> int:
        """Ingest all docs for a single library.
        
        1. List all files in raw/docs/{library}/ recursively
        2. For each file:
           a. Read content
           b. Chunk it
           c. Collect all chunks
        3. Batch insert into ChromaDB (batches of 100 chunks)
           - Use the collection name "technical_docs"
           - The ChromaDBHandler should use the SentenceTransformer model to generate
             embeddings automatically (ChromaDB supports custom embedding functions)
        4. Print progress: "Ingested {library}: {n_chunks} chunks from {n_files} files"
        5. Return total chunks ingested
        """
        ...

    async def ingest_all(self) -> dict[str, int]:
        """Ingest docs for all libraries.
        
        Returns: {"langchain": 500, "fastapi": 300, "python": 100}
        """
        ...

    async def clear_and_reingest(self) -> dict[str, int]:
        """Delete the collection and re-ingest everything from scratch.
        
        Useful during development.
        """
        ...
```

---

## ChromaDB Handler Updates (`src/db/chromadb.py`)

The existing `ChromaDBHandler` needs updates to support batch operations and the custom embedding model properly.

### Required Changes:

1. **Fix the bug at line 57**: `db = ChromaDB()` should be `db = ChromaDBHandler()` or removed entirely (use DI instead).

2. **Custom Embedding Function**: ChromaDB supports custom embedding functions. Create one that wraps `SentenceTransformer`:

```python
from chromadb import Documents, EmbeddingFunction, Embeddings
from sentence_transformers import SentenceTransformer

class SentenceTransformerEmbedding(EmbeddingFunction):
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self._model = SentenceTransformer(model_name)

    def __call__(self, input: Documents) -> Embeddings:
        embeddings = self._model.encode(input, show_progress_bar=False)
        return embeddings.tolist()
```

3. **Batch add method**:

```python
def add_documents_batch(
    self,
    ids: list[str],
    texts: list[str],
    metadatas: list[dict] | None = None,
    collection_name: str = "technical_docs",
) -> None:
    """Add multiple documents at once.
    
    ChromaDB has a batch limit of ~5461 items per add() call.
    If len(ids) > 5000, split into sub-batches.
    """
    collection = self.get_collection(collection_name)
    batch_size = 5000
    for i in range(0, len(ids), batch_size):
        batch_end = min(i + batch_size, len(ids))
        collection.add(
            ids=ids[i:batch_end],
            documents=texts[i:batch_end],
            metadatas=metadatas[i:batch_end] if metadatas else None,
        )
```

4. **Pass the custom embedding function** to `get_or_create_collection`:

```python
def get_collection(self, name: str = "technical_docs") -> chromadb.Collection:
    if name not in self._collections:
        self._collections[name] = self.client.get_or_create_collection(
            name=name,
            embedding_function=self._embedding_fn,
        )
    return self._collections[name]
```

5. **Remove `Optional` import** — use `| None` syntax (Python 3.13).

---

## Chunking Strategy — Detailed Rules

### For Markdown (.md, .mdx):

1. **Parse headers**: Use regex to find lines starting with `#`, `##`, `###`, etc.
2. **Create sections**: Each header starts a new section. The section includes everything until the next header of same or higher level.
3. **Handle code blocks**: Track opening/closing ``` markers. Never split inside a code block.
4. **MDX cleanup**: Remove JSX-like tags: `<Tabs>`, `<TabItem>`, `{import ...}`, etc. Use regex: `<[A-Z][a-zA-Z]*[^>]*>.*?</[A-Z][a-zA-Z]*>` (multiline). Also remove import statements at the top.
5. **Max chunk size**: ~4000 characters (~1000 tokens for MiniLM). If a section exceeds this, split by paragraphs with 200-char overlap.
6. **Min chunk size**: 100 characters. Merge small chunks with the next one.
7. **Strip excessive whitespace**: Collapse multiple blank lines into one.

### For RST (.rst):

1. **Convert RST headers**: RST uses underlines (`====`, `----`, `~~~~`). Convert to markdown `#`, `##`, `###`.
2. **Remove RST directives**: Lines starting with `.. ` (like `.. code-block:: python`, `.. note::`). Keep the content inside directives.
3. **Remove role markup**: `:ref:\`text\`` → `text`, `:func:\`os.path\`` → `os.path`.
4. **Then apply the same chunking as Markdown**.

---

## Metadata Schema

Every chunk MUST have this metadata structure:

```python
{
    "library": str,          # "langchain" | "fastapi" | "python"
    "source_file": str,      # Relative path within raw/docs/{library}/
    "section": str,          # Top-level heading (H1) or filename if no H1
    "subsection": str,       # Current section heading (H2/H3) or "intro" if before first header
    "chunk_index": int,      # 0-indexed position within the file
    "total_chunks": int,     # Total chunks from this file
    "char_count": int,       # Character count of this chunk
    "has_code": bool,        # True if chunk contains ``` code blocks (stored as string "true"/"false" because ChromaDB metadata only supports str, int, float, bool)
}
```

**Note**: ChromaDB metadata values must be `str`, `int`, `float`, or `bool`. No nested objects or lists.

---

## Verification

After running ingest:

1. `collection.count()` should return 500+ documents (rough estimate)
2. Run a test query: `collection.query(query_texts=["How to create a FastAPI endpoint"], n_results=3)` — results should contain FastAPI docs
3. Run a filtered query: `collection.query(query_texts=["dependency injection"], n_results=3, where={"library": "fastapi"})` — should return only FastAPI results
4. Check metadata is present on all results
5. No duplicate IDs (running ingest twice should not increase count)

---

## Dependencies

No new dependencies needed. Uses:
- `sentence-transformers` (already in pyproject.toml)
- `chromadb` (already in pyproject.toml)
- Standard library: `pathlib`, `re`, `os`
